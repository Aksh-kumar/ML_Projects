{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Friday\n",
    "### Problen Statement\n",
    "A retail company “ABC Private Limited” wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.\n",
    "The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.\n",
    "Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import all necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import xgboost as xgb\n",
    "import types\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(r'./Data/train.csv')\n",
    "dataset_test = pd.read_csv(r'./Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check first 5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550068\n",
      "233599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender   Age  Occupation City_Category  \\\n",
       "0  1000001  P00069042      F  0-17          10             A   \n",
       "1  1000001  P00248942      F  0-17          10             A   \n",
       "2  1000001  P00087842      F  0-17          10             A   \n",
       "3  1000001  P00085442      F  0-17          10             A   \n",
       "4  1000002  P00285442      M   55+          16             C   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "0                          2               0                   3   \n",
       "1                          2               0                   1   \n",
       "2                          2               0                  12   \n",
       "3                          2               0                  12   \n",
       "4                         4+               0                   8   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0                 NaN                 NaN      8370  \n",
       "1                 6.0                14.0     15200  \n",
       "2                 NaN                 NaN      1422  \n",
       "3                14.0                 NaN      1057  \n",
       "4                 NaN                 NaN      7969  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_test))\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store categorical variables column name and dictionary with column to unique value mapping associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender ['F' 'M']\n",
      "Age ['0-17' '18-25' '26-35' '36-45' '46-50' '51-55' '55+']\n",
      "Occupation [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "City_Category ['A' 'B' 'C']\n",
      "Stay_In_Current_City_Years ['0' '1' '2' '3' '4+']\n",
      "Marital_Status [0 1]\n",
      "Product_Category_1 [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Product_Category_2 [ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. nan]\n",
      "Product_Category_3 [ 3.  4.  5.  6.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. nan]\n"
     ]
    }
   ],
   "source": [
    "columns = dataset_train.columns\n",
    "dic_columnwise_acceped_value = {}\n",
    "for i in columns[2:-1] :\n",
    "    temp1 = dataset_train[i].unique()\n",
    "    temp2 = dataset_test[i].unique()\n",
    "    try :\n",
    "        if np.isnan(temp1).any() and np.isnan(temp2).any() :\n",
    "            temp1 = temp1[~np.isnan(temp1)]\n",
    "    except TypeError :\n",
    "        pass\n",
    "    tem_dup =  np.hstack([temp1, temp2])\n",
    "    tem_dup = np.unique(tem_dup)\n",
    "    dic_columnwise_acceped_value[i] = list(tem_dup)\n",
    "    print(i,tem_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check and print if any column contain null or NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ID                            0\n",
      "Product_ID                         0\n",
      "Gender                             0\n",
      "Age                                0\n",
      "Occupation                         0\n",
      "City_Category                      0\n",
      "Stay_In_Current_City_Years         0\n",
      "Marital_Status                     0\n",
      "Product_Category_1                 0\n",
      "Product_Category_2            173638\n",
      "Product_Category_3            383247\n",
      "Purchase                           0\n",
      "dtype: int64\n",
      "User_ID                            0\n",
      "Product_ID                         0\n",
      "Gender                             0\n",
      "Age                                0\n",
      "Occupation                         0\n",
      "City_Category                      0\n",
      "Stay_In_Current_City_Years         0\n",
      "Marital_Status                     0\n",
      "Product_Category_1                 0\n",
      "Product_Category_2             72344\n",
      "Product_Category_3            162562\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.isna().sum())\n",
    "print(dataset_test.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case product_category_2 and product_category_3 contain null value\n",
    "Create SimpleImputer class to replace NaN to 999 so that later on it can be HotEncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_NaN(data, columns, *args) :    \n",
    "    mp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=999)\n",
    "    transformed_val = mp.fit_transform(data.iloc[:,[columns.get_loc(i) for i in list(args)]].values)\n",
    "    df = data.copy()\n",
    "    df[list(args)] = transformed_val\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing Nan from two columns Product_Category_2 Product_Category_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = replace_NaN(dataset_train, dataset_train.columns, 'Product_Category_2', 'Product_Category_3')\n",
    "dataset_test = replace_NaN(dataset_test, dataset_test.columns, 'Product_Category_2', 'Product_Category_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00069042</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00248942</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00087842</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000001</td>\n",
       "      <td>P00085442</td>\n",
       "      <td>F</td>\n",
       "      <td>0-17</td>\n",
       "      <td>10</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>P00285442</td>\n",
       "      <td>M</td>\n",
       "      <td>55+</td>\n",
       "      <td>16</td>\n",
       "      <td>C</td>\n",
       "      <td>4+</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>7969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000003</td>\n",
       "      <td>P00193542</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>15</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>15227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender    Age  Occupation City_Category  \\\n",
       "0  1000001  P00069042      F   0-17          10             A   \n",
       "1  1000001  P00248942      F   0-17          10             A   \n",
       "2  1000001  P00087842      F   0-17          10             A   \n",
       "3  1000001  P00085442      F   0-17          10             A   \n",
       "4  1000002  P00285442      M    55+          16             C   \n",
       "5  1000003  P00193542      M  26-35          15             A   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "0                          2               0                   3   \n",
       "1                          2               0                   1   \n",
       "2                          2               0                  12   \n",
       "3                          2               0                  12   \n",
       "4                         4+               0                   8   \n",
       "5                          3               0                   1   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0               999.0               999.0      8370  \n",
       "1                 6.0                14.0     15200  \n",
       "2               999.0               999.0      1422  \n",
       "3                14.0               999.0      1057  \n",
       "4               999.0               999.0      7969  \n",
       "5                 2.0               999.0     15227  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each categorical column we created n-1 number of dummy variable which contain either 0 or 1\n",
    "create a function which take data, column Names list, column name (In which we want to perform an operation)\n",
    "and remove column name which we want to remove in order to avoid dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_column_with_Dummy_Columns(data, columns, column_name, remove_column_val, remove_one_dummy=False, dtype=int) :\n",
    "    temp = pd.get_dummies(data[column_name], prefix=column_name, dtype=dtype)\n",
    "    col = list(temp.columns)\n",
    "    removed_col = column_name+'_'+remove_column_val\n",
    "    removed_col_index = col.index(removed_col)\n",
    "    temp = temp[col[:removed_col_index] + col[removed_col_index+(1 if remove_one_dummy else 0):]] # removing to avoid dummy variable trap\n",
    "    previous = data[columns[:columns.index(column_name)]]\n",
    "    after = data[columns[columns.index(column_name)+1:]]\n",
    "    previous = previous.join(temp)\n",
    "    previous = previous.join(after)\n",
    "    return previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For One categorical column of training data create hotencoded column from function replace_column_with_Dummy_Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_dataset(data, columns, cat_col_list) :\n",
    "    col = cat_col_list\n",
    "    df = data[list(filter(lambda x: x not in cat_col_list, columns))]\n",
    "    for i in cat_col_list :\n",
    "        dic = data[i].value_counts().to_dict()\n",
    "        max_key = max(dic, key=dic.get) # for non column removal it automatically contain maximim number\n",
    "        # so we dont need to remove it seperately\n",
    "        data = replace_column_with_Dummy_Columns(data, col, i, str(max_key), True)\n",
    "        col = list(data.columns)\n",
    "    df= df.join(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through each categorical column in training and testing set to get dummy coded column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_dataset(dataset, category_column) :\n",
    "    columns = list(dataset.columns)\n",
    "    df = get_dummy_dataset(dataset, columns, category_column)\n",
    "    return df\n",
    "def get_label_encoded_data(df, label_col) :\n",
    "    # label encode data\n",
    "    for i in label_col :\n",
    "        le = LabelEncoder()\n",
    "        df[i] = le.fit_transform(df[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create hot encoded data for training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df, slice_index, data_label) : # for both numpy and dataframe\n",
    "    if type(data_label) is not pd.DataFrame :\n",
    "        assert False\n",
    "    if type(df) is pd.DataFrame :\n",
    "        sliced_train_data = df.iloc[:slice_index,:]\n",
    "        sliced_test_data = df.iloc[slice_index:, :]\n",
    "        sliced_train_data = pd.concat([sliced_train_data, data_label], axis = 1)\n",
    "    else :\n",
    "        data_label = data_label.iloc[:, :].values\n",
    "        sliced_train_data = df[:slice_index]\n",
    "        sliced_test_data = df[slice_index:]\n",
    "        sliced_train_data = np.concatenate((sliced_train_data, data_label), axis = 1)\n",
    "    return sliced_train_data, sliced_test_data\n",
    "def get_encoded_data(dataset_train, dataset_test, model_type='non_tree_based') :\n",
    "    column = list(dataset_train.columns)\n",
    "    df = dataset_train[column[:-1]] # seperate label column\n",
    "    df = pd.concat([df, dataset_test], ignore_index=True) # merge 2 dataframe\n",
    "    category_column = ['Gender', 'Occupation', 'City_Category',\n",
    "                       'Product_Category_1', 'Product_Category_2',\n",
    "                       'Product_Category_3']\n",
    "    label_col = ['Age', 'Stay_In_Current_City_Years']\n",
    "    if model_type == 'non_tree_based' :\n",
    "        df = get_label_encoded_data(df, label_col)\n",
    "        df = get_encoded_dataset(df, category_column)\n",
    "    elif model_type == 'tree_based' :\n",
    "        df = get_label_encoded_data(df, label_col+category_column)\n",
    "    else :\n",
    "        pass\n",
    "    return df\n",
    "def PCR_feature_selection_graph(df) :\n",
    "    pca = PCA().fit(df)\n",
    "    #Plotting the Cumulative Summation of the Explained Variance\n",
    "    plt.figure()\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance (%)') #for each component\n",
    "    plt.title('Black friday Explained Variance')\n",
    "    plt.show()\n",
    "def PCR_feature_selection(df, n_comp) :\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    return pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components that are dropped correspond to the near collinearities. Consequently, the standard errors of the parameter estimates are reduced, although the tradeoff is that the estimates are biased, and \"the bias increases as more  principal components are dropped so we are not applying principal component analysis.we seperate our data transformation in two different sections \n",
    "1. Non tree based which uses hot and label encoded data\n",
    "2. Tree based model which only uses label encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_model_based(dataset_train, dataset_test, model_type='non_tree_based') :\n",
    "    data_label = dataset_train[['Purchase']]\n",
    "    slice_index = len(dataset_train)\n",
    "    if model_type == 'non_tree_based' :\n",
    "        df = get_encoded_data(dataset_train, dataset_test, model_type)\n",
    "        df = df.drop(['User_ID', 'Product_ID'], axis = 1)\n",
    "        # PCA_feature_selection_graph(df)\n",
    "        #df = PCA_feature_selection(df, 60)\n",
    "        data_train, data_test = get_train_test_data(df, slice_index, data_label)\n",
    "    elif  model_type == 'tree_based' :\n",
    "        df = get_encoded_data(dataset_train, dataset_test, model_type)\n",
    "        df = df.drop(['User_ID', 'Product_ID'], axis = 1)\n",
    "        df = df.applymap(str)\n",
    "        data_train, data_test = get_train_test_data(df, slice_index, data_label)\n",
    "    else  :\n",
    "        return None, None\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non tree based Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = get_data_model_based(dataset_train, dataset_test, model_type='non_tree_based')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tree based Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatb_train, datatb_test = get_data_model_based(dataset_train, dataset_test, model_type='tree_based')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create generic model object which store all the Information of related model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelObject :\n",
    "    def __init__(self, name) :\n",
    "        self._name = name\n",
    "        self._cross_validation_score = None\n",
    "        self._model = None\n",
    "    @property\n",
    "    def name(self) :\n",
    "        return self._name\n",
    "    @property\n",
    "    def cross_validation_score(self) :\n",
    "        return self._cross_validation_score\n",
    "    @cross_validation_score.setter\n",
    "    def cross_validation_score(self, value) :\n",
    "        try :\n",
    "            self._cross_validation_score = value\n",
    "        except Exception as e:\n",
    "            raise Exception('value object is not in format',e)\n",
    "    @property\n",
    "    def model(self) :\n",
    "        return self._model\n",
    "    @model.setter\n",
    "    def model(self, value) :\n",
    "        self._model = value\n",
    "        self._cross_validation_score = None\n",
    "    def __str__(self) :\n",
    "        res = '\\n'\n",
    "        res += 'Model Name :- ' + self._name + '\\n'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data_test doesn't contain target variable thats why I am splitting data_train to X_train, y_train, X_test and y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a function which return train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_data(data) :\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    X_train, X_test, y_train , y_test = model_selection.train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "    return (X_train, X_test, y_train , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models\n",
    "### 1.Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_modellr(X_train, y_train) :\n",
    "    seed_k_fold = 7\n",
    "    no_of_split = 10\n",
    "    scoring = 'neg_mean_squared_error'\n",
    "    lr = ModelObject('Linear Regression')\n",
    "    kfold = model_selection.KFold(n_splits= no_of_split, random_state=seed_k_fold)\n",
    "    lr.model = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
    "    cv_results = -model_selection.cross_val_score(lr.model, X_train, y_train,cv=kfold, scoring=scoring)\n",
    "    \"\"\"The unified scoring API always maximizes the score, so scores which need to be minimized are negated\n",
    "    in order for the unified scoring API to work correctly. The score that is returned is therefore negated\n",
    "    when it is a score that should be minimized and left positive if it is a score that should be maximized.\"\"\"\n",
    "    cv_results.sort(axis=-1, kind='mergesort', order=None)\n",
    "    lr.cross_validation_score = cv_results\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get linear model without any regularization and check its RMSE as the performance metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr mean - 8890732.525132675\n"
     ]
    }
   ],
   "source": [
    "data = get_test_train_data(data_train.values)\n",
    "if data is not None :\n",
    "    X_train, X_test, y_train , y_test = data[0], data[1], data[2], data[3]\n",
    "    lr = get_optimal_modellr(X_train, y_train)\n",
    "    print('cr mean -',lr.cross_validation_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply regularizationo technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ridge_regression_model(lambda_val) :\n",
    "    return Ridge(alpha = lambda_val, fit_intercept=True, normalize=False, copy_X=True,\n",
    "                         max_iter=None, tol=0.001, solver='auto', random_state=0)\n",
    "def get_optimal_model_ridge(X_train, y_train, verbose=True, lambda_start=0.001, lambda_stop=1.2, no_split=10) :\n",
    "    seed_k_fold = 7\n",
    "    no_of_split = no_split\n",
    "    range_lambda  = np.logspace(lambda_start, lambda_stop, num=no_split)\n",
    "    kf = model_selection.KFold(n_splits= no_of_split, random_state=seed_k_fold)\n",
    "    index = 0\n",
    "    val_score_list = []\n",
    "    #Applying cross validation for getting appropriate value of lambda\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_tr, X_tst = X_train[train_index], X_train[test_index]\n",
    "        y_tr, y_tst = y_train[train_index], y_train[test_index]\n",
    "        temp = get_ridge_regression_model(range_lambda[index])\n",
    "        temp.fit(X_tr,y_tr)\n",
    "        predicted = temp.predict(X_tst)\n",
    "        validation_score = mean_squared_error(y_tst, predicted)\n",
    "        val_score_list.append(validation_score)\n",
    "        if verbose :\n",
    "            print('lambda - '+str(range_lambda[index]) + '--validation score '+str(validation_score))\n",
    "        index += 1\n",
    "    min_val_score_index = val_score_list.index(min(val_score_list))\n",
    "    lambda_optimal = range_lambda[min_val_score_index]\n",
    "    ridge_reg = ModelObject('Ridge Regression')\n",
    "    ridge_reg.model = get_ridge_regression_model(lambda_optimal)\n",
    "    ridge_reg.cross_validation_score = val_score_list[min_val_score_index]\n",
    "    return ridge_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda - 1.0023052380778996--validation score 8925627.527068092\n",
      "lambda - 1.362141492331366--validation score 8844921.419810327\n",
      "lambda - 1.8511620758251646--validation score 8845271.176420026\n",
      "lambda - 2.515745280696361--validation score 8826084.63764821\n",
      "lambda - 3.4189196073092853--validation score 8842036.4666963\n",
      "lambda - 4.646341333097262--validation score 8896723.662220791\n",
      "lambda - 6.314418080347418--validation score 8870320.828464543\n",
      "lambda - 8.581348815120236--validation score 8950951.27574684\n",
      "lambda - 11.66212730131956--validation score 9031182.293514831\n",
      "lambda - 15.848931924611133--validation score 8873092.101679886\n",
      "8886996.47578433\n"
     ]
    }
   ],
   "source": [
    "# fitting optimal model\n",
    "rr = get_optimal_model_ridge(X_train, y_train, verbose=True)\n",
    "rr.model.fit(X_train,y_train)\n",
    "predicted = rr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lasso_regression_model(lambda_val) :\n",
    "    # precompute - Whether to use a precomputed Gram matrix to speed up calculations\n",
    "    # warm_start - When set to True, reuse the solution of the previous call to fit as initialization\n",
    "    return Lasso(alpha = lambda_val, fit_intercept=True, normalize=False, precompute=False,\n",
    "                            copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False,\n",
    "                            random_state=0, selection='cyclic')\n",
    "def get_optimal_model_lasso(X_train, y_train, verbose=True, lambda_start=0.001, lambda_stop=1.2, no_split=10) :\n",
    "    seed_k_fold = 7\n",
    "    no_of_split = no_split\n",
    "    range_lambda  = np.logspace(lambda_start, lambda_stop, num=no_split)\n",
    "    kf = model_selection.KFold(n_splits= no_of_split, random_state=seed_k_fold)\n",
    "    index = 0\n",
    "    val_score_list = []\n",
    "    #Applying cross validation for getting appropriate value of lambda\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_tr, X_tst = X_train[train_index], X_train[test_index]\n",
    "        y_tr, y_tst = y_train[train_index], y_train[test_index]\n",
    "        temp =  get_lasso_regression_model(range_lambda[index])\n",
    "        temp.fit(X_tr, y_tr)\n",
    "        predicted = temp.predict(X_tst)\n",
    "        validation_score = mean_squared_error(y_tst, predicted)\n",
    "        val_score_list.append(validation_score)\n",
    "        if verbose :\n",
    "            print('lambda - '+str(range_lambda[index]) + '--validation score '+str(validation_score))\n",
    "        index += 1\n",
    "    min_val_score_index = val_score_list.index(min(val_score_list))\n",
    "    lambda_optimal = range_lambda[min_val_score_index]\n",
    "    lasso_reg = ModelObject('Lasso Regression')\n",
    "    lasso_reg.model = get_lasso_regression_model(lambda_optimal)\n",
    "    lasso_reg.cross_validation_score = val_score_list[min_val_score_index]\n",
    "    return lasso_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda - 1.0023052380778996--validation score 8936631.365144437\n",
      "lambda - 1.362141492331366--validation score 8853675.972363977\n",
      "lambda - 1.8511620758251646--validation score 8875320.468876196\n",
      "lambda - 2.515745280696361--validation score 8877660.431314765\n",
      "lambda - 3.4189196073092853--validation score 8944085.185055582\n",
      "lambda - 4.646341333097262--validation score 9060190.162094783\n",
      "lambda - 6.314418080347418--validation score 9123042.470936557\n",
      "lambda - 8.581348815120236--validation score 9318790.540123885\n",
      "lambda - 11.66212730131956--validation score 9491816.247081403\n",
      "lambda - 15.848931924611133--validation score 9497639.233877284\n",
      "8901754.690498047\n"
     ]
    }
   ],
   "source": [
    "# fitting optimal model\n",
    "lr = get_optimal_model_lasso(X_train, y_train, verbose=True)\n",
    "lr.model.fit(X_train,y_train)\n",
    "predicted = lr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Support vector regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svr_model(c=1.0) :\n",
    "    #C = it is a hyperparameter that controls how much we penalize our use of slack variables\n",
    "    #slack variable - as a value ζthat, roughly, indicates how much we must move our point so that\n",
    "    #it is correctly and confidently classified\n",
    "    return LinearSVR(epsilon=0.0, tol=0.0001, C=c, loss='epsilon_insensitive', fit_intercept=True,\n",
    "                     intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)\n",
    "def get_optimal_model_svr(X_train, y_train, c_start=14, c_stop=33, verbose=True) :\n",
    "    range_c  = np.linspace(c_start, c_stop, num=8)\n",
    "    X_tr, X_tst, y_tr, y_tst = model_selection.train_test_split(X_train, y_train, test_size = 0.1, random_state=0)\n",
    "    val_score_list = []\n",
    "    #Applying cross validation for getting appropriate value of c\n",
    "    for i in range_c:\n",
    "        temp =  get_svr_model(i)\n",
    "        temp.fit(X_tr, y_tr)\n",
    "        predicted = temp.predict(X_tst)\n",
    "        validation_score = mean_squared_error(y_tst, predicted)\n",
    "        val_score_list.append(validation_score)\n",
    "        if verbose :\n",
    "            print('c - '+str(i) + '--validation score '+str(validation_score))\n",
    "    min_val_score_index = val_score_list.index(min(val_score_list))\n",
    "    c_optimal = range_c[min_val_score_index]\n",
    "    svr_reg = ModelObject('SVR')\n",
    "    svr_reg.model = get_svr_model(c_optimal)\n",
    "    svr_reg.cross_validation_score = val_score_list[min_val_score_index]\n",
    "    return svr_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c - 14.0--validation score 9740006.926035345\n",
      "c - 16.714285714285715--validation score 9724116.276088322\n",
      "c - 19.42857142857143--validation score 9737061.310684197\n",
      "c - 22.142857142857142--validation score 9735489.389833312\n",
      "c - 24.857142857142858--validation score 9715213.616381284\n",
      "c - 27.571428571428573--validation score 9708551.151698155\n",
      "c - 30.285714285714285--validation score 9716244.617435142\n",
      "c - 33.0--validation score 9705327.730926773\n",
      "9762564.69189564\n"
     ]
    }
   ],
   "source": [
    "# fitting optimal model\n",
    "svr = get_optimal_model_svr(X_train, y_train)\n",
    "svr.model.fit(X_train,y_train)\n",
    "predicted = svr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.k-nearest neighbors Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knr_model(k=5) :\n",
    "    algo = 'auto' #{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}\n",
    "    return KNeighborsRegressor(n_neighbors=k, weights='uniform', algorithm=algo, leaf_size=30, p=2,\n",
    "                               metric='minkowski', metric_params=None, n_jobs=-1)\n",
    "def get_optimal_model_knr(X_train, y_train,k_start=1, k_stop=5, verbose=True) :\n",
    "    range_k  = np.arange(k_start,k_stop+1,dtype=int)\n",
    "    X_tr, X_tst, y_tr, y_tst = model_selection.train_test_split(X_train, y_train, test_size = 0.1, random_state=0)\n",
    "    val_score_list = []\n",
    "    #Applying cross validation for getting appropriate value of k\n",
    "    for j in range_k:\n",
    "        temp = get_knr_model(j)\n",
    "        temp.fit(X_tr, y_tr)\n",
    "        predicted = temp.predict(X_tst)\n",
    "        validation_score = mean_squared_error(y_tst, predicted)\n",
    "        val_score_list.append(validation_score)\n",
    "        if verbose :\n",
    "            print('k - '+str(j) + '--validation score '+str(validation_score))\n",
    "    knr_reg = ModelObject('KNR')\n",
    "    k_optimal = val_score_list.index(min(val_score_list)) + 1\n",
    "    knr_reg.model = get_knr_model(k_optimal)\n",
    "    knr_reg.cross_validation_score = val_score_list[k_optimal-1]\n",
    "    return knr_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k - 1--validation score 16758322.596668636\n",
      "k - 2--validation score 12796929.822882107\n",
      "k - 3--validation score 11675475.985835265\n",
      "k - 4--validation score 11203160.269369518\n",
      "k - 5--validation score 10986796.915093398\n",
      "k - 6--validation score 10925121.449397052\n",
      "k - 7--validation score 10888404.722758586\n",
      "k - 8--validation score 10881650.094229693\n",
      "k - 9--validation score 10891254.60540089\n",
      "k - 10--validation score 10923843.527569193\n",
      "k - 11--validation score 10941208.553442938\n",
      "k - 12--validation score 11014805.403263347\n",
      "k - 13--validation score 11061729.665181616\n",
      "k - 14--validation score 11094325.79974507\n",
      "10963517.764887594\n"
     ]
    }
   ],
   "source": [
    "knr = get_optimal_model_knr(X_train, y_train, k_start=1, k_stop=14, verbose=True)\n",
    "knr.model.fit(X_train,y_train)\n",
    "predicted = knr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get data for tress based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_test_train_data(datatb_train.values)\n",
    "if data is not None :\n",
    "    X_train, X_test, y_train , y_test = data[0], data[1], data[2], data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '2', '7', '2', '4', '1', '7', '17', '15'],\n",
       "       ['1', '2', '14', '1', '2', '0', '7', '15', '15']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_tree_model( max_depth = None, min_samples_lf = 1, min_samples_splt = 2, verbose = True) :\n",
    "    criteria = 'mse'\n",
    "    split = 'best'\n",
    "    dt = DecisionTreeRegressor(criterion=criteria, splitter=split, max_depth=max_depth,\n",
    "                                 min_samples_split=min_samples_splt, min_samples_leaf=min_samples_lf,\n",
    "                                 min_weight_fraction_leaf=0.0,max_features=None, random_state=0,\n",
    "                                 max_leaf_nodes=None,min_impurity_decrease=0.0,\n",
    "                                 min_impurity_split=None, presort=False)\n",
    "    if verbose :\n",
    "        print('max depth {0} , minimum sample leaf - {1} minimum sample split {2}'.format(str(max_depth), \n",
    "                                                                                          str(min_samples_lf),\n",
    "                                                                                         str(min_samples_splt)))\n",
    "    dt_reg = ModelObject('Decision Tree')\n",
    "    dt_reg.model = dt\n",
    "    return dt_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max depth 14 , minimum sample leaf - 10 minimum sample split 10\n",
      "8691766.743108112\n"
     ]
    }
   ],
   "source": [
    "dtr = get_decision_tree_model(14, 10, 10)\n",
    "dtr.model.fit(X_train, y_train)\n",
    "predicted = dtr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_regressor_model(no_of_trees = 10, max_depth = None, min_samples_lf = 1,\n",
    "                                              min_samples_splt = 2, verbose = True) :\n",
    "    #no_of_trees, max_depth, min_samples_lf, min_samples_splt  = 15, None, 10, 10\n",
    "    criteria = 'mse'\n",
    "    rf = RandomForestRegressor(n_estimators=no_of_trees, criterion=criteria, max_depth=max_depth, min_samples_split=min_samples_splt,\n",
    "                                 min_samples_leaf=min_samples_lf, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                                 max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 bootstrap=True, oob_score=False, n_jobs=None, random_state=0, verbose=0,\n",
    "                                 warm_start=False)\n",
    "    #temp =  get_random_forest_regressor_model(no_of_trees, max_depth, min_samples_lf, min_samples_splt)\n",
    "    #temp.fit(X_train, y_train)\n",
    "    if verbose :\n",
    "        print('No of trees {0} max depth {1} , minimum sample leaf - {2} minimum sample split {3}'.format(str(no_of_trees),\n",
    "                                                                                            str(max_depth), \n",
    "                                                                                          str(min_samples_lf),\n",
    "                                                                                         str(min_samples_splt)))\n",
    "    rf_reg = ModelObject('Random Forest Regressor')\n",
    "    rf_reg.model = rf\n",
    "    return rf_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of trees 10 max depth None , minimum sample leaf - 1 minimum sample split 2\n",
      "9516116.01576707\n"
     ]
    }
   ],
   "source": [
    "rfr = get_random_forest_regressor_model()\n",
    "rfr.model.fit(X_train, y_train)\n",
    "predicted = rfr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_of_trees = 10\n",
    "# max depth None , minimum sample leaf - 10 minimum sample split 10   8375037.457358595\n",
    "# max depth 8 , minimum sample leaf - 10 minimum sample split 10     10564781.562420707\n",
    "# max depth 10 , minimum sample leaf - 10 minimum sample split 10     9739102.832783798\n",
    "# max depth 12 , minimum sample leaf - 10 minimum sample split 10     9236034.181042949"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ADA Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12216741.593958858\n"
     ]
    }
   ],
   "source": [
    "def get_ADABoost_regressor_model(n_est=1000, l_r=1.0) :\n",
    "    _loss = 'linear' #‘square’, ‘exponential’}, optional (default=’linear’)\n",
    "    #base_estimator If None, then the base estimator is DecisionTreeRegressor(max_depth=3)\n",
    "    ada =  AdaBoostRegressor(base_estimator=None, n_estimators=n_est, learning_rate=l_r, loss=_loss, random_state=0)\n",
    "    ada_reg = ModelObject('ADA Boost Regressor')\n",
    "    ada_reg.model = ada\n",
    "    return ada_reg\n",
    "ada = get_ADABoost_regressor_model()\n",
    "ada.model.fit(X_train, y_train)\n",
    "predicted = ada.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8216126.166656249\n"
     ]
    }
   ],
   "source": [
    "def get_gradient_boosting_regressor_model(n_est=100, max_dpt = 8, min_leaf = 5, l_r=0.1) :\n",
    "    loss = 'ls' # ls - least squares regression {ls’, ‘lad’, ‘huber’, ‘quantile’}\n",
    "    gbr =  GradientBoostingRegressor(loss=loss, learning_rate=l_r, n_estimators=n_est, subsample=1.0, criterion='mse',\n",
    "                                     min_samples_split=2,min_samples_leaf=min_leaf, min_weight_fraction_leaf=0.0,\n",
    "                                     max_depth=max_dpt, min_impurity_decrease=0.0,min_impurity_split=None, init=None,\n",
    "                                     random_state=0, max_features=None, alpha=0.9,verbose=0, max_leaf_nodes=None,\n",
    "                                     warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None,\n",
    "                                     tol=0.0001)\n",
    "    gbr_reg = ModelObject('Gradient Boost Regressor')\n",
    "    gbr_reg.model = gbr\n",
    "    return gbr_reg\n",
    "gbr = get_gradient_boosting_regressor_model()\n",
    "gbr.model.fit(X_train, y_train)\n",
    "predicted = gbr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8239876.402650073\n"
     ]
    }
   ],
   "source": [
    "def get_XBGoosting_regressor_model(base_scr, num_rounds = 3000, max_dpth = 6, min_chld_wt=10) :\n",
    "    param = {}\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['verbosity'] = 0\n",
    "    param['eta '] = 0.3\n",
    "    param['gamma'] = 0\n",
    "    param['max_depth'] = max_dpth\n",
    "    param['min_child_weight'] = min_chld_wt\n",
    "    param['colsample_bytree'] = 0.7 # subsample ratio of columns when constructing each tree\n",
    "    param['lambda'] = 1\n",
    "    param['alpha'] = 0\n",
    "    param['scale_pos_weight'] = 0.8 # Control the balance of positive and negative weights, useful for unbalanced classes.\n",
    "    #A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "    param['objective'] = 'reg:squarederror'\n",
    "    param['base_score'] = base_scr\n",
    "    param['eval_metric'] = 'rmse'\n",
    "    param['seed'] = 0\n",
    "    def train_xgb(X_train, y_train) : # creating closure for generic for train test data\n",
    "        xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        xgb_m = xgb.train(param, xgtrain, num_rounds)\n",
    "        xgb_reg = ModelObject('XGBoost Regressor')\n",
    "        xgb_reg.model = xgb_m\n",
    "        return xgb_reg\n",
    "    return train_xgb\n",
    "xgbr = get_XBGoosting_regressor_model(y_train.mean())\n",
    "predicted = xgbr(X_train, y_train).model.predict( xgb.DMatrix(X_test))\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Extra Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_tree_regressor_model(no_of_trees = 10, max_depth = None, min_samples_lf = 1,\n",
    "                                              min_samples_splt = 2, verbose = True) :\n",
    "    #no_of_trees, max_depth, min_samples_lf, min_samples_splt  = 15, None, 10, 10\n",
    "    criteria = 'mse'\n",
    "    et = ExtraTreesRegressor(n_estimators=no_of_trees, criterion=criteria, max_depth=max_depth, min_samples_split=min_samples_splt,\n",
    "                                 min_samples_leaf=min_samples_lf, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                                 max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                 bootstrap=True, oob_score=False, n_jobs=None, random_state=0, verbose=0,\n",
    "                                 warm_start=False)\n",
    "    #temp.fit(X_train, y_train)\n",
    "    if verbose :\n",
    "        print('No of trees {0} max depth {1} , minimum sample leaf - {2} minimum sample split {3}'.format(str(no_of_trees),\n",
    "                                                                                            str(max_depth), \n",
    "                                                                                          str(min_samples_lf),\n",
    "                                                                                         str(min_samples_splt)))\n",
    "    et_reg = ModelObject('Extra Tree Regressor')\n",
    "    et_reg.model = et\n",
    "    return et_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of trees 10 max depth None , minimum sample leaf - 1 minimum sample split 2\n",
      "9492625.38208102\n"
     ]
    }
   ],
   "source": [
    "etr = get_extra_tree_regressor_model()\n",
    "etr.model.fit(X_train, y_train)\n",
    "predicted = etr.model.predict(X_test)\n",
    "print(mean_squared_error(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating model based on Trees i.e Decision trees, Random Forest , ADA Boost, Gradient Boost, Extra Trees,\n",
    "XGBoost model with different values of Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = get_decision_tree_model( max_depth = 10, min_samples_lf = 10, min_samples_splt = 10, verbose = False)\n",
    "model_2 = get_decision_tree_model( max_depth = 12, min_samples_lf = 10, min_samples_splt = 10, verbose = False)\n",
    "model_3 = get_decision_tree_model( max_depth = 14, min_samples_lf = 10, min_samples_splt = 10, verbose = False)\n",
    "model_4 = get_random_forest_regressor_model(no_of_trees = 500, max_depth = 10, min_samples_lf = 10,min_samples_splt = 10,\n",
    "                                            verbose = False)\n",
    "model_5 = get_random_forest_regressor_model(no_of_trees = 1000, max_depth = 12, min_samples_lf = 10,min_samples_splt = 10,\n",
    "                                            verbose = False)\n",
    "model_6 = get_random_forest_regressor_model(no_of_trees = 1500, max_depth = 14, min_samples_lf = 10,min_samples_splt = 10,\n",
    "                                            verbose = False)\n",
    "model_7 = get_extra_tree_regressor_model(no_of_trees = 500, max_depth = 10, min_samples_lf = 10,min_samples_splt = 10,\n",
    "                                            verbose = False)\n",
    "model_8 = get_extra_tree_regressor_model(no_of_trees = 1000, max_depth = 12, min_samples_lf = 10,min_samples_splt = 10,\n",
    "                                            verbose = False)\n",
    "model_9 = get_extra_tree_regressor_model(no_of_trees = 1500, max_depth = 14, min_samples_lf = 10,min_samples_splt = 10,\n",
    "                                            verbose = False)\n",
    "model_10 = get_ADABoost_regressor_model(n_est=1000, l_r=1.0)\n",
    "# return a function because don't want to create any dependency of training and testing data\n",
    "model_11 = lambda base_scr : get_XBGoosting_regressor_model(base_scr, num_rounds = 3000, max_dpth = 6, min_chld_wt=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating 1st level prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelsPrediction :\n",
    "    def __init__(self, *models) :\n",
    "        self.models = list(models)\n",
    "        self.gboost_model = {}\n",
    "    def fit_models(self, X_tr, y_tr, verbose = False) :\n",
    "        print('start fitting {0} model one by one'.format(len(self.models)))\n",
    "        counter = 1\n",
    "        for m in self.models :\n",
    "            print(counter)\n",
    "            if isinstance(m, types.FunctionType) :\n",
    "                base_scr = y_tr.mean()\n",
    "                temp = m(base_scr)\n",
    "                model = temp(X_tr, y_tr)\n",
    "                self.gboost_model[self.models.index(m)] = model\n",
    "                if verbose :\n",
    "                    print('fitting {0} model completed:----'.format(model.name))\n",
    "            else :\n",
    "                model = m.model.fit(X_tr, y_tr)\n",
    "                if verbose :\n",
    "                    print('fitting {0} model completed:----'.format(m.name))\n",
    "            counter += 1\n",
    "    def predict(self, X_tst, verbose= False) :\n",
    "        tp = ()\n",
    "        counter = 1\n",
    "        for m in self.models :\n",
    "            print(counter)\n",
    "            if isinstance(m, types.FunctionType) :\n",
    "                temp = self.gboost_model[self.models.index(m)]\n",
    "                predicted = temp.model.predict( xgb.DMatrix(X_tst))\n",
    "                if verbose :\n",
    "                    print('model Name - {0} prediction completed'.format(temp.name))\n",
    "            else :\n",
    "                predicted = m.model.predict(X_tst)\n",
    "                if verbose :\n",
    "                    print('model Name - {0} prediction completed'.format(m.name))\n",
    "            counter += 1\n",
    "            tp += (predicted, )\n",
    "        return np.vstack(tp).T # create column wise prediction value of length n_sample, number of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create first level prediction\n",
    "flp = ModelsPrediction(model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10, model_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit multiple model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting 11 model one by one\n",
      "1\n",
      "fitting Decision Tree model completed:----\n",
      "2\n",
      "fitting Decision Tree model completed:----\n",
      "3\n",
      "fitting Decision Tree model completed:----\n",
      "4\n",
      "fitting Random Forest Regressor model completed:----\n",
      "5\n",
      "fitting Random Forest Regressor model completed:----\n",
      "6\n",
      "fitting Random Forest Regressor model completed:----\n",
      "7\n",
      "fitting Extra Tree Regressor model completed:----\n",
      "8\n",
      "fitting Extra Tree Regressor model completed:----\n",
      "9\n",
      "fitting Extra Tree Regressor model completed:----\n",
      "10\n",
      "fitting ADA Boost Regressor model completed:----\n",
      "11\n",
      "fitting XGBoost Regressor model completed:----\n"
     ]
    }
   ],
   "source": [
    "flp.fit_models(X_train, y_train, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict train data in order to get feature for second level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "model Name - Decision Tree prediction completed\n",
      "2\n",
      "model Name - Decision Tree prediction completed\n",
      "3\n",
      "model Name - Decision Tree prediction completed\n",
      "4\n",
      "model Name - Random Forest Regressor prediction completed\n",
      "5\n",
      "model Name - Random Forest Regressor prediction completed\n",
      "6\n",
      "model Name - Random Forest Regressor prediction completed\n",
      "7\n",
      "model Name - Extra Tree Regressor prediction completed\n",
      "8\n",
      "model Name - Extra Tree Regressor prediction completed\n",
      "9\n",
      "model Name - Extra Tree Regressor prediction completed\n",
      "10\n",
      "model Name - ADA Boost Regressor prediction completed\n",
      "11\n",
      "model Name - XGBoost Regressor prediction completed\n"
     ]
    }
   ],
   "source": [
    "X_test_second_level = flp.predict(X_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating second level model XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_level_model(x_base, X_tr, y_tr, num_rounds = 1500, max_dpth = 8, min_chld_wt=10, verbose = False) :\n",
    "    kf = KFold(n_splits= 2, shuffle=False, random_state=0)\n",
    "    for train_index, test_index in kf.split(X_tr):\n",
    "        X_train, X_test = X_tr[train_index], X_tr[test_index]\n",
    "        y_train, y_test = y_tr[train_index], y_tr[test_index]\n",
    "        xgbr = get_XBGoosting_regressor_model(y_train.mean(), num_rounds = 1500, max_dpth = 8, min_chld_wt=10)\n",
    "        predicted = xgbr(X_train, y_train).model.predict( xgb.DMatrix(X_test))\n",
    "        mse = mean_squared_error(y_test, predicted)\n",
    "    # create second level meta feature with all the meta data\n",
    "    xgbr = get_XBGoosting_regressor_model(y_tr.mean(), num_rounds = 1500, max_dpth = 8, min_chld_wt=10)\n",
    "    model = xgbr(X_tr, y_tr)\n",
    "    return model\n",
    "sl = get_second_level_model(y_test.mean(), X_test_second_level, y_test, num_rounds = 1500, max_dpth = 8,\n",
    "                            min_chld_wt=10, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "model Name - Decision Tree prediction completed\n",
      "2\n",
      "model Name - Decision Tree prediction completed\n",
      "3\n",
      "model Name - Decision Tree prediction completed\n",
      "4\n",
      "model Name - Random Forest Regressor prediction completed\n",
      "5\n",
      "model Name - Random Forest Regressor prediction completed\n",
      "6\n",
      "model Name - Random Forest Regressor prediction completed\n",
      "7\n",
      "model Name - Extra Tree Regressor prediction completed\n",
      "8\n",
      "model Name - Extra Tree Regressor prediction completed\n",
      "9\n",
      "model Name - Extra Tree Regressor prediction completed\n",
      "10\n",
      "model Name - ADA Boost Regressor prediction completed\n",
      "11\n",
      "model Name - XGBoost Regressor prediction completed\n"
     ]
    }
   ],
   "source": [
    "first_level_prediction = flp.predict(datatb_test.values, True)\n",
    "final_prediction = sl.model.predict(xgb.DMatrix(first_level_prediction) , True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store result to test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>City_Category</th>\n",
       "      <th>Stay_In_Current_City_Years</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Product_Category_1</th>\n",
       "      <th>Product_Category_2</th>\n",
       "      <th>Product_Category_3</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000004</td>\n",
       "      <td>P00128942</td>\n",
       "      <td>M</td>\n",
       "      <td>46-50</td>\n",
       "      <td>7</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>19659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000009</td>\n",
       "      <td>P00113442</td>\n",
       "      <td>M</td>\n",
       "      <td>26-35</td>\n",
       "      <td>17</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>10323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000010</td>\n",
       "      <td>P00288442</td>\n",
       "      <td>F</td>\n",
       "      <td>36-45</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>4+</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000010</td>\n",
       "      <td>P00145342</td>\n",
       "      <td>F</td>\n",
       "      <td>36-45</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>4+</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000011</td>\n",
       "      <td>P00053842</td>\n",
       "      <td>F</td>\n",
       "      <td>26-35</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID Product_ID Gender    Age  Occupation City_Category  \\\n",
       "0  1000004  P00128942      M  46-50           7             B   \n",
       "1  1000009  P00113442      M  26-35          17             C   \n",
       "2  1000010  P00288442      F  36-45           1             B   \n",
       "3  1000010  P00145342      F  36-45           1             B   \n",
       "4  1000011  P00053842      F  26-35           1             C   \n",
       "\n",
       "  Stay_In_Current_City_Years  Marital_Status  Product_Category_1  \\\n",
       "0                          2               1                   1   \n",
       "1                          0               0                   3   \n",
       "2                         4+               1                   5   \n",
       "3                         4+               1                   4   \n",
       "4                          1               0                   4   \n",
       "\n",
       "   Product_Category_2  Product_Category_3  Purchase  \n",
       "0                11.0               999.0     19659  \n",
       "1                 5.0               999.0     10323  \n",
       "2                14.0               999.0      8500  \n",
       "3                 9.0               999.0      2647  \n",
       "4                 5.0                12.0      2787  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test['Purchase'] = np.rint(final_prediction).astype(np.int32)\n",
    "dataset_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
