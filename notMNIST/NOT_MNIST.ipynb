{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from PIL import Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting constants used\n",
    "num_classes = 10 # total number of classes\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel\n",
    "np.random.seed(133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_file(filename):\n",
    "    global num_classes\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0] # remove tar.gz\n",
    "    if os.path.isdir(root):\n",
    "        pass\n",
    "    else:\n",
    "        print('Extraction of data for %s this may take while please wait...' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        \n",
    "    data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "                                    if os.path.isdir(os.path.join(root, d))]\n",
    "    \n",
    "    assert len(data_folders) == num_classes\n",
    "    return data_folders\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'notMNIST_large.tar.gz'\n",
    "test_filename = 'notMNIST_small.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = extract_tar_file(train_filename)\n",
    "test_folders = extract_tar_file(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_letter_data(folder, min_number_of_image):\n",
    "    # load datafor a single letter label\n",
    "    image_files = os.listdir(folder)\n",
    "    #dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32)\n",
    "    data = []\n",
    "    #num_images = 0\n",
    "    \n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            # image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            image_data = (np.array(Image.open(image_file)).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                print('UnExpected Image shape {0} skipping file {1}'.format(str(image_data.shape), image))\n",
    "                continue\n",
    "            data.append({'ImagePath' : image,\n",
    "                        'Dimension': image_size,\n",
    "                        'Feature': image_data,\n",
    "                        'Label': folder})\n",
    "            #num_images += 1\n",
    "        except IOError as er:\n",
    "            print('couldn \\'t read ', image_file, ':', er)\n",
    "    return data\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_pickle(data_folders: str, min_num_images_per_class: int, force=False) ->list:\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        \n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already present skipping pickling.'%set_filename)\n",
    "        else:\n",
    "            print('pickling %s '%set_filename)\n",
    "            dataset = load_letter_data(folder, min_num_images_per_class)\n",
    "            #print(dataset)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except IOError as e:\n",
    "                print('unable to save the data to %s'%set_filename, ':', e)\n",
    "    return dataset_names\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickling notMNIST_large\\A.pickle \n",
      "couldn 't read  notMNIST_large\\A\\RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file 'notMNIST_large\\\\A\\\\RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png'\n",
      "couldn 't read  notMNIST_large\\A\\SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file 'notMNIST_large\\\\A\\\\SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png'\n",
      "couldn 't read  notMNIST_large\\A\\Um9tYW5hIEJvbGQucGZi.png : cannot identify image file 'notMNIST_large\\\\A\\\\Um9tYW5hIEJvbGQucGZi.png'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c1c415ef9ee3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_folders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_folders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-50fedf368e2e>\u001b[0m in \u001b[0;36mmaybe_pickle\u001b[1;34m(data_folders, min_num_images_per_class, force)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pickling %s '\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mset_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_letter_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_num_images_per_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;31m#print(dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-010e15970c57>\u001b[0m in \u001b[0;36mload_letter_data\u001b[1;34m(folder, min_number_of_image)\u001b[0m\n\u001b[0;32m     25\u001b[0m                                    \u001b[1;34m'Dimension'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                                    \u001b[1;34m'Feature'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                                    'Label': np.array([folder]*n)})\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpandas_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# End\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    410\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         ]\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# from BlockManager perspective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    321\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             val = sanitize_array(\n\u001b[1;32m--> 323\u001b[1;33m                 \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m             )\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "train_datasets = maybe_pickle(train_folders, 1, True)\n",
    "test_datasets = maybe_pickle(test_folders, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(pkl_dataset_file: list) ->dict:\n",
    "    res = {}\n",
    "    for dataset in pkl_dataset_file:\n",
    "        with open(dataset,'rb') as f:\n",
    "            res[dataset.replace('\\\\', '.').split('.')[1]] = pickle.load(f)\n",
    "    return res\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = load_dataset(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_array(nb_rows, img_size):\n",
    "    dataset , label = None, None\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        label = np.ndarray(nb_rows, dtype=np.float32)\n",
    "    return dataset, label\n",
    "#End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(pickles_files, train_size, valid_size=0):\n",
    "    global image_size\n",
    "    num_classes = len(pickles_files)\n",
    "    valid_dataset, valid_labels = make_array(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_array(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickles_files):\n",
    "        try:\n",
    "            with open(pickle_file) as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_latter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v: end_v ,:, :] = valid_latter\n",
    "                    valid_labels[start_v: end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                train_latter = letter_set[vsize_per_class: end_l, :, :]\n",
    "                train_dataset[start_t: end_t ,:, :] = train_latter\n",
    "                train_labels[start_t: end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print(\"not able to process the data from \", pickle_file,':', e)\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not able to process the data from  notMNIST_large\\A.pickle : 'charmap' codec can't decode byte 0x9d in position 1081: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\B.pickle : 'charmap' codec can't decode byte 0x81 in position 1020: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\C.pickle : 'charmap' codec can't decode byte 0x9d in position 368: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\D.pickle : 'charmap' codec can't decode byte 0x90 in position 881: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\E.pickle : 'charmap' codec can't decode byte 0x90 in position 696: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\F.pickle : 'charmap' codec can't decode byte 0x8d in position 284: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\G.pickle : 'charmap' codec can't decode byte 0x8d in position 3781: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\H.pickle : 'charmap' codec can't decode byte 0x81 in position 1020: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\I.pickle : 'charmap' codec can't decode byte 0x8f in position 5024: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_large\\J.pickle : 'charmap' codec can't decode byte 0x8f in position 2608: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\A.pickle : 'charmap' codec can't decode byte 0x8d in position 528: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\B.pickle : 'charmap' codec can't decode byte 0x8f in position 292: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\C.pickle : 'charmap' codec can't decode byte 0x90 in position 341: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\D.pickle : 'charmap' codec can't decode byte 0x81 in position 276: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\E.pickle : 'charmap' codec can't decode byte 0x9d in position 304: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\F.pickle : 'charmap' codec can't decode byte 0x9d in position 288: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\G.pickle : 'charmap' codec can't decode byte 0x9d in position 201: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\H.pickle : 'charmap' codec can't decode byte 0x8d in position 352: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\I.pickle : 'charmap' codec can't decode byte 0x81 in position 304: character maps to <undefined>\n",
      "not able to process the data from  notMNIST_small\\J.pickle : 'charmap' codec can't decode byte 0x9d in position 344: character maps to <undefined>\n",
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
