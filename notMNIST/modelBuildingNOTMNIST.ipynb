{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dataPreprocessingNOTNIST.ipynb data preprocessing is done\n",
    "in this notebook we start with normalization and model creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set constants\n",
    "NUM_CLASSES = 10 # total number of classes\n",
    "IMAGE_SIZE = 28  # Pixel width and height.\n",
    "PIXEL_DEPTH = 255.0  # Number of levels per pixel\n",
    "SEED = 42 # for permutation in data shuffuling\n",
    "TRAIN_SIZE_PER_LABEL = 30000 # take TRAIN_SIZE_PER_LABEL data for training from each sample\n",
    "VALIDATION_SIZE_PER_LABEL = 20000 # take VALIDATION_SIZE_PER_LABEL amount for validation from each sample\n",
    "TEST_SIZE = -1 # Negative Number represent All Data will be taken \n",
    "test_suffix = 'all' if TEST_SIZE < 0 else TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_pickle_filepath = 'tr_set_{0}_val_{1}_test_{2}.pickle'.format(TRAIN_SIZE_PER_LABEL,\n",
    "                                                                 VALIDATION_SIZE_PER_LABEL,\n",
    "                                                                 test_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_set_30000_val_20000_test_all.pickle\n"
     ]
    }
   ],
   "source": [
    "print(ds_pickle_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from already preprocessed pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath: str)->object:\n",
    "    \"\"\"\n",
    "    load the data set from given pickle file\n",
    "    \n",
    "    paramter\n",
    "    --------\n",
    "        filepath: str\n",
    "        pickle file path\n",
    "    \n",
    "    return pickle object\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise Exception('pickle file is not present..')\n",
    "        \n",
    "    dataset = None\n",
    "    with open(filepath, 'rb') as f:\n",
    "        try:\n",
    "            dataset = pickle.load(f)\n",
    "        except EOFError:\n",
    "            raise\n",
    "    return dataset\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset comes in dictionary form Like\n",
    "<pre>\n",
    "{\n",
    "    'trainingSet30000': training_set, # 30000 represent number of data of each sample\n",
    "    'trainingLabel30000': training_label, # 30000 represent number of label of each sample\n",
    "    'validationSet20000': validation_set, # 20000 represent number of data of each sample\n",
    "    'validationLabel20000': validation_label, # 20000 represent number of label of each sample\n",
    "    'testSetall': test_set, # all test data sample is used\n",
    "    'testLabelall': test_label,  # all test data labels sample is used\n",
    "    'classLabelEncodingMapping': CLASS_MAP # represent class label to integer mapping in label encoding\n",
    "}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(ds_pickle_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = dataset['trainingSet'+ str(TRAIN_SIZE_PER_LABEL)]\n",
    "training_label = dataset['trainingLabel'+ str(TRAIN_SIZE_PER_LABEL)]\n",
    "validation_set = dataset['validationSet'+ str(VALIDATION_SIZE_PER_LABEL)]\n",
    "validation_label = dataset['validationLabel'+ str(VALIDATION_SIZE_PER_LABEL)]\n",
    "test_set = dataset['testSetall']\n",
    "test_label = dataset['testLabelall']\n",
    "CLASS_MAP = dataset['classLabelEncodingMapping']\n",
    "CLASS_INV_MAP = {val: key for key, val in CLASS_MAP.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training set    :- (300000, 28, 28)\n",
      "shape of training label  :- (300000, 10)\n",
      "shape of validation set  :- (200000, 28, 28)\n",
      "shape of validation label:- (200000, 10)\n",
      "shape of test set        :- (18724, 28, 28)\n",
      "shape of training label  :- (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "print('shape of training set    :-',training_set.shape)\n",
    "print('shape of training label  :-', training_label.shape)\n",
    "print('shape of validation set  :-',validation_set.shape)\n",
    "print('shape of validation label:-', validation_label.shape)\n",
    "print('shape of test set        :-',test_set.shape)\n",
    "print('shape of training label  :-', test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets are randomize but not normalized we have to normalized all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_data(dataset: np.array)->np.array:\n",
    "    \"\"\"\n",
    "     normalized and centered to have Local Centering mean is very small\n",
    "     centers the pixel values\n",
    "     \n",
    "     parameter\n",
    "     ---------\n",
    "     \n",
    "    dataset: np.array\n",
    "         data in numpy array\n",
    "    \n",
    "    returns\n",
    "    -------\n",
    "        normalized np.array \n",
    "    \n",
    "    \"\"\"\n",
    "    global PIXEL_DEPTH\n",
    "    \n",
    "    def norm(x: np.array)->np.array:\n",
    "        \"\"\"\n",
    "        function for normalising each image data\n",
    "        \"\"\"\n",
    "        x_ = x.astype('float64')/255.0\n",
    "        nrm = x_ - x_.mean()\n",
    "        return nrm\n",
    "    # End\n",
    "        \n",
    "    data = np.array(list(map(norm, dataset)))\n",
    "    return data\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and center the pixels values the train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_val, X_test =  normalised_data(training_set), normalised_data(validation_set), normalised_data(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to flaten the image array data since it is 2 dimension data for each image (IMAGE_SIZE, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_flat = X_tr.reshape(X_tr.shape[0], IMAGE_SIZE*IMAGE_SIZE)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], IMAGE_SIZE*IMAGE_SIZE)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], IMAGE_SIZE*IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training set   :- (300000, 784)\n",
      "shape of validation set :- (200000, 784)\n",
      "shape of test set       :- (18724, 784)\n"
     ]
    }
   ],
   "source": [
    "print('shape of training set   :-', X_tr_flat.shape)\n",
    "print('shape of validation set :-', X_val_flat.shape)\n",
    "print('shape of test set       :-', X_test_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression Model SkLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Logistic regression first to see How traditional algorithm perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since label is one hot encoded but we need label encoding\n",
    "so we convert label data in label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_int_label = np.array(list(map(np.argmax, training_label)))\n",
    "val_int_label =  np.array(list(map(np.argmax, validation_label)))\n",
    "test_int_label = np.array(list(map(np.argmax, test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_reg_filename(model: LogisticRegression, train_size: int)->str:\n",
    "    \"\"\"\n",
    "    return the filename with path to save logistic regression model\n",
    "    \n",
    "    paramter\n",
    "    --------\n",
    "        model: sklearn.linear_model._logistic.LogisticRegression\n",
    "            sklearn logistic regression model\n",
    "        train_size: int train sample size of each class\n",
    "    \n",
    "    return\n",
    "    ------\n",
    "        str -> pickle file path filepath\n",
    "    \"\"\"\n",
    "    hs = hashlib.sha256(str(model.get_params()).encode('utf-8')).hexdigest()\n",
    "    filename = 'logistic_reg_{0}_{1}.pickle'.format(train_size, hs)\n",
    "    return filename\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logistic_regression_model(model: LogisticRegression, train_size: int,\n",
    "                                   isalreadyfitted: bool, force: bool=False)->None:\n",
    "    \"\"\"\n",
    "    save pickled logistic model information into file\n",
    "    \n",
    "    paramter\n",
    "    --------\n",
    "    model:  sklearn.linear_model._logistic.LogisticRegression\n",
    "        sklearn logistic regression model\n",
    "    \n",
    "    train_size: int  \n",
    "        number of training samples (per labels)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    filepath = get_log_reg_filename(model, train_size)\n",
    "    if os.path.exists(filepath) and not force:\n",
    "        print('{0} file is already present skipping it...'.format(filepath))\n",
    "        return\n",
    "    \n",
    "    data =  {\n",
    "                'trainSampleSize': train_size,\n",
    "                'modelParameter': model.get_params(),\n",
    "                'model': model,\n",
    "                'isAlreadyFitted': isalreadyfitted\n",
    "            }\n",
    "    print('saving data to pickle file')\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression_model(params: Dict[str, object], train_size: str)->Tuple[LogisticRegression, bool]:\n",
    "    \"\"\"\n",
    "    return logistic regression model If pickled model already found return from file else create new one\n",
    "        \n",
    "    parameter\n",
    "    ---------\n",
    "        params: Dict[str, object]\n",
    "            parameter for logistic regression model\n",
    "        train_size: int\n",
    "            number of training samples (per labels)\n",
    "        \n",
    "    return\n",
    "    -------\n",
    "        Tuple[sklearn.linear_model._logistic.LogisticRegression. bool] -> sklearn logistic regression model, is already fitted\n",
    "    \"\"\"\n",
    "    \n",
    "    logistic_reg_model = LogisticRegression()\n",
    "    logistic_reg_model.set_params(**params)\n",
    "    filename = get_log_reg_filename(logistic_reg_model, train_size)\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        data = None\n",
    "        print('saved Model Found loading from pickle file..')\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        return (data['model'], data['isAlreadyFitted'])\n",
    "    else:    \n",
    "        print('saved model not found returning new model..')\n",
    "        return (logistic_reg_model, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up the required parameter for Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {\n",
    "                'penalty':'l2',\n",
    "                 'C':1.0,\n",
    "                 'solver':'lbfgs',\n",
    "                 'max_iter':1000,\n",
    "                 'multi_class':'multinomial',\n",
    "                 'verbose':1,\n",
    "                 'n_jobs':-1\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the model and check if loaded model is already trained or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved Model Found loading from pickle file..\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='multinomial', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=1,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "logistic_reg_model, is_fitted = get_logistic_regression_model(parameter, TRAIN_SIZE_PER_LABEL)\n",
    "print(logistic_reg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if model is not trained before train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_fitted:\n",
    "    logistic_reg_model.fit(X_tr_flat, tr_int_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving model into file if already saved then it will skip, set force = True to overide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_reg_30000_ebc80f6d2e18afef7fba8e3cbcc7c4c9df2152b31f4472d7045801787c845943.pickle file is already present skipping it...\n"
     ]
    }
   ],
   "source": [
    "save_logistic_regression_model(model=logistic_reg_model, train_size=TRAIN_SIZE_PER_LABEL, isalreadyfitted=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have validation and test set seperately because I am skipping hyper parameter tuning which \n",
    "require validation set and its label since it is very much time consuming you can create a\n",
    "function to parameter tuning and save only the best model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_and_predicted_label(data: np.array, true_label: np.array)->Tuple[np.array, float]:\n",
    "    \"\"\"\n",
    "    return predicted labels and accuracy score \n",
    "    \n",
    "    paramter\n",
    "    --------\n",
    "        data np.array\n",
    "            flatted and normalised np.array of IMAGE_SIZE*IMAGE*SIZE dimesion\n",
    "        true_label: np.array:\n",
    "            flattern label encoded true label\n",
    "    \n",
    "    return\n",
    "    -------\n",
    "        Tuple[np.array, float]- > (predicted label encoded array, accuracy score)\n",
    "    \"\"\"\n",
    "    y_pred = logistic_reg_model.predict(data)\n",
    "    score = accuracy_score(true_label, y_pred)\n",
    "    return (y_pred, score)\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_pred, score_validation = get_score_and_predicted_label(X_val_flat, val_int_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 81.1635%\n",
      "total misclassified sample  37673\n",
      "total correct classified sample  162327\n"
     ]
    }
   ],
   "source": [
    "print('accuracy is {0}%'.format(score_validation*100))\n",
    "misclassified_validation = np.sum(val_int_label != validation_pred)\n",
    "correct_classified_validation = len(val_int_label) - misclassified_validation\n",
    "print('total misclassified sample ', misclassified_validation)\n",
    "print('total correct classified sample ', correct_classified_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred, score_test = get_score_and_predicted_label(X_test_flat, test_int_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 89.43067720572527\n",
      "total misclassified sample  1979\n",
      "total correct classified sample  16745\n"
     ]
    }
   ],
   "source": [
    "print('accuracy is {0}'.format(score_test*100))\n",
    "misclassified_test = np.sum(test_int_label != test_pred)\n",
    "correct_classified_test = len(test_int_label) - misclassified_test\n",
    "print('total misclassified sample ', misclassified_test)\n",
    "print('total correct classified sample ', correct_classified_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic regression using simplae gradient descent in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['Variable:0', 'Variable_1:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a347a4f68a23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mtrain_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[0;32m    316\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_compute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    424\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m--> 426\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[1;32m-> 1039\u001b[1;33m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[0;32m   1040\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     logging.warning(\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['Variable:0', 'Variable_1:0']."
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    global IMAGE_SIZE, NUM_CLASSES, SEED\n",
    "    learning_rate = 0.5\n",
    "    ############### Data sets conversion into tensorflow constant ##########################\n",
    "    tf_train_dataset = tf.constant(X_tr_flat, dtype=tf.float64)\n",
    "    tf_valid_dataset = tf.constant(X_val_flat, dtype=tf.float64)\n",
    "    tf_test_dataset = tf.constant(X_test_flat, dtype=tf.float64)\n",
    "    tf_train_label = tf.constant(training_label, dtype=tf.float64)\n",
    "    tf_validation_label = tf.constant(validation_label, dtype=tf.float64)\n",
    "    \n",
    "    ###################### weight matrix and Biases initialization ####################################\n",
    "    \n",
    "    # Initialized By using random values following a (truncated) normal distribution\n",
    "    weights = tf.Variable(tf.random.truncated_normal((IMAGE_SIZE * IMAGE_SIZE, NUM_CLASSES),\n",
    "                                                     seed=tf.random.set_seed(SEED), dtype=tf.float64),\n",
    "                                                     dtype=tf.float64, trainable=True)\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES] , dtype=tf.float64), trainable=True)\n",
    "    \n",
    "    # multiply weight matrix and add Bias\n",
    "    logit = tf.add(tf.matmul(tf_train_dataset, weights), biases)\n",
    "    \n",
    "    # generate loss function\n",
    "    # labels: Each row labels[i] must be a valid probability distribution\n",
    "    # logits: Unscaled log probabilities.\n",
    "    loss =  lambda : tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_label, logits=logit))\n",
    "    \n",
    "    # optimizer\n",
    "    #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate).minimize(loss, var_list=[weights,biases])\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logit)\n",
    "    valid_prediction = tf.nn.softmax(tf.nn.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn,softmax(tf.nn.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_computation_graph(X, y, X_val, y_val, X_test):\n",
    "    graph = tf.Graph()\n",
    "    IMAGE_SIZE, NUM_CLASSES, SEED = 28, 10, 42\n",
    "    learning_rate = 0.5\n",
    "    \n",
    "    with graph.as_default():    \n",
    "        ############### Data sets conversion into tensorflow constant ##########################\n",
    "        tf_train_dataset = tf.constant(X, dtype=tf.float64)\n",
    "        tf_valid_dataset = tf.constant(X_val, dtype=tf.float64)\n",
    "        tf_test_dataset = tf.constant(X_test, dtype=tf.float64)\n",
    "        tf_train_label = tf.constant(y, dtype=tf.float64)\n",
    "        tf_validation_label = tf.constant(y_val, dtype=tf.float64)\n",
    "\n",
    "        ###################### weight matrix and Biases initialization ####################################\n",
    "\n",
    "        # Initialized By using random values following a (truncated) normal distribution\n",
    "        weights = tf.Variable(tf.random.truncated_normal((IMAGE_SIZE * IMAGE_SIZE, NUM_CLASSES),\n",
    "                                                         seed=tf.random.set_seed(SEED), dtype=tf.float64),\n",
    "                                                         dtype=tf.float64, trainable=True)\n",
    "\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES] , dtype=tf.float64), trainable=True)\n",
    "\n",
    "        # multiply weight matrix and add Bias\n",
    "        logit = tf.add(tf.matmul(tf_train_dataset, weights), biases)\n",
    "\n",
    "        # generate loss function\n",
    "        # labels: Each row labels[i] must be a valid probability distribution\n",
    "        # logits: Unscaled log probabilities.\n",
    "        loss =  lambda : tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_label, logits=logit))\n",
    "\n",
    "        # optimizer\n",
    "        #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate).minimize(loss, var_list=[weights,biases])\n",
    "\n",
    "        train_prediction = tf.nn.softmax(logit)\n",
    "        valid_prediction = tf.nn.softmax(tf.nn.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction = tf.nn,softmax(tf.nn.matmul(tf_test_dataset, weights) + biases)\n",
    "        return graph\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y, X_v, y_v, X_t = X_tr_flat[:10000], training_label[: 10000],  X_val_flat[: 2000], validation_label[: 2000], X_test_flat[: 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape     (10000, 784)\n",
      "training labels shape   (10000, 10)\n",
      "validation data shape   (2000, 784)\n",
      "validation labels shape (2000, 10)\n",
      "test data shape         (4000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('training data shape    ', X.shape)\n",
    "print('training labels shape  ', y.shape)\n",
    "print('validation data shape  ', X_v.shape)\n",
    "print('validation labels shape', y_v.shape)\n",
    "print('test data shape        ', X_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['Variable:0', 'Variable_1:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-311f05332c36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbuild_computation_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-f0e7793c1da2>\u001b[0m in \u001b[0;36mbuild_computation_graph\u001b[1;34m(X, y, X_val, y_val, X_test)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m#optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mtrain_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[0;32m    316\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_compute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    424\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m--> 426\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[1;32m-> 1039\u001b[1;33m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[0;32m   1040\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     logging.warning(\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['Variable:0', 'Variable_1:0']."
     ]
    }
   ],
   "source": [
    "build_computation_graph(X, y, X_v, y_v, X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data_Sub/data_subset.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_v, y_v, X_t = data['X'], data['y'], data['X_v'], data['y_v'], data['X_t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datal['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
