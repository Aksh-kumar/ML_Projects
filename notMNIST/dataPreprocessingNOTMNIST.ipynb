{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not MNIST Data Classifiaction\n",
    "Given a <a href='http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html'> Data set </a> of not MNIST Data\n",
    "which have 10 classes of letter A-J our Job os to classify the Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.image as tfpreprocess\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the constants train folder and test folder can be easily obtain by extracting the notMNIST dataset\n",
    "from the link provided at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting constants used\n",
    "NUM_CLASSES = 10 # total number of classes\n",
    "IMAGE_SIZE = 28  # Pixel width and height.\n",
    "PIXEL_DEPTH = 255.0  # Number of levels per pixel\n",
    "PATH_TRAIN = []\n",
    "SEED = 42 # for permutation in data shuffuling\n",
    "PATH_TEST = []\n",
    "TRAIN_FOLDER = 'notMNIST_large' # Folder that contain all the alphabets image folder wise \n",
    "TEST_FOLDER = 'notMNIST_small' # Folder that contain all the alphabets image folder wise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check number of sub folder is equal to NUM_CLASS (number of subfolder in train folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_directory = list(os.listdir(TRAIN_FOLDER))\n",
    "for sub in sub_directory:\n",
    "    merge_path = os.path.join(TRAIN_FOLDER, sub)\n",
    "    if not os.path.isfile(merge_path):\n",
    "        PATH_TRAIN.append(merge_path)   # add mapping  merge path to class Name tuple\n",
    "assert len(PATH_TRAIN) == NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load all the sub path of test data note it may may not be equal to the NUM_CLASS\n",
    "since it is not mendatory all classes data are present in test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_directory = list(os.listdir(TEST_FOLDER))\n",
    "for sub in sub_directory:\n",
    "    merge_path = os.path.join(TEST_FOLDER, sub)\n",
    "    if not os.path.isfile(merge_path):\n",
    "        PATH_TEST.append(merge_path)  # add mapping  merge path to class Name tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "object form of an image containing Information About Images and Its vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickel_object(image_path: str, extension: str, data: object,\n",
    "                                  width: int, height: int, label: str)->object:\n",
    "    \"\"\"\n",
    "    Get Pickle object which contain all relative Information of Image\n",
    "    \n",
    "    parameter\n",
    "    ----------\n",
    "    \n",
    "    image_path: str\n",
    "        path of the Image from where It has been loaded\n",
    "    \n",
    "    extension: str\n",
    "        extension of Image .png, .jpg, e.t.c\n",
    "        \n",
    "    data: str\n",
    "        vector if (IMAGE_SIZE, IMAGE_SIZE) number representing grayscale pixels value\n",
    "    \n",
    "    width: int\n",
    "        width of the Image\n",
    "    \n",
    "    height: int\n",
    "        height of the Image\n",
    "    \n",
    "    label; string\n",
    "        Label of the Image which It belongs\n",
    "    \n",
    "    \"\"\"\n",
    "    return {\n",
    "            'imagePath': image_path,\n",
    "            'extension': extension,\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'data': data,\n",
    "            'label': label\n",
    "           }\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Augumented Image From random sample Image List of Data with Random Sample behaviour ans Also Save\n",
    "The Augumented Image If save_augumented_image set it to true in the same directory of image sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augumentation_generator(data: List[object], max_count: int, save_augumented_image=False)->List[object]:\n",
    "    \"\"\"\n",
    "    Generate augumented Image of till data length not equal to max_count passes from paramter\n",
    "    \n",
    "    paramter\n",
    "    -----------\n",
    "    \n",
    "    data: List[object]\n",
    "        list of objects (containing property {'imagePath' ,'extension', 'width', 'height', 'data', 'label'})\n",
    "        \n",
    "    max_count: int\n",
    "        maximum number of augumented image can be created (max_count - len(data) augumented image data)\n",
    "    \n",
    "    save_augumented_image: bool\n",
    "        If true then augumented Image is saved to Disk In the same directry from which random sample from data list image is taken\n",
    "        saved Image format is  [imagelabel]_[imagename]_augumented_image_[number].[extension]\n",
    "    \n",
    "    returns\n",
    "    ----------\n",
    "        List[object]\n",
    "        object list of augumented Images\n",
    "    \"\"\"\n",
    "    ############################  Image Generator ###########################################\n",
    "    image_gen = [tfpreprocess.ImageDataGenerator(zoom_range=0.2),\n",
    "                 tfpreprocess.ImageDataGenerator(horizontal_flip=True, vertical_flip=True),\n",
    "                 tfpreprocess.ImageDataGenerator(zoom_range=0.5),\n",
    "                 tfpreprocess.ImageDataGenerator(vertical_flip=True),\n",
    "                 tfpreprocess.ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.1),\n",
    "                 tfpreprocess.ImageDataGenerator(rotation_range=45),\n",
    "                 tfpreprocess.ImageDataGenerator(rotation_range=90),\n",
    "                 tfpreprocess.ImageDataGenerator(width_shift_range=0.3, height_shift_range=0.1, rotation_range=90, vertical_flip=True),\n",
    "                 tfpreprocess.ImageDataGenerator(width_shift_range=0.23, height_shift_range=0.01, rotation_range=90),\n",
    "                 tfpreprocess.ImageDataGenerator(channel_shift_range=0.9),\n",
    "                 tfpreprocess.ImageDataGenerator(channel_shift_range=0.9, rotation_range=90)\n",
    "                ]\n",
    "    \n",
    "    n, n_igen = len(data), len(image_gen)\n",
    "    diff = max_count - n\n",
    "    ls = []\n",
    "    \n",
    "    for i in range(1, diff+1):\n",
    "        img_gen_idx = np.random.randint(n_igen)\n",
    "        image_selected_idx = np.random.randint(n)\n",
    "        img_gen_selected = image_gen[img_gen_idx]\n",
    "        image = data[image_selected_idx]\n",
    "        datagen = img_gen_selected.flow(image['data'].reshape(1, 1, IMAGE_SIZE, IMAGE_SIZE), batch_size = 1)\n",
    "        new_image_v = datagen.next()[0].astype('uint8').reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "        img_path_split =  os.path.split(image['imagePath'])\n",
    "        img_name_split = os.path.splitext(img_path_split[1])\n",
    "        \n",
    "        if save_augumented_image:\n",
    "            image_name = '{0}_{1}_augumented_image_{2}{3}'.format(image['label'], img_name_split[0], str(i), img_name_split[1])\n",
    "            directory = img_path_split[0]\n",
    "            image_path = os.path.join(directory, image_name)\n",
    "            image_save = tfpreprocess.array_to_img(new_image_v.reshape(28, 28, 1))\n",
    "            image_save.save(image_path)\n",
    "        else:\n",
    "            image_path = ''\n",
    "        obj = get_pickel_object(image_path, image['extension'], new_image_v,\n",
    "                                    IMAGE_SIZE, IMAGE_SIZE, image['label'])\n",
    "        ls.append(obj)\n",
    "    \n",
    "    return ls\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to retrieve data from sub folder i.e. notMNISTlarge/A or  notMNISTlarge/B e.t.c\n",
    "dataset create List of object obtained from get_pickel_object() function and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_path(image_folder: str, label: str)->List[object]:\n",
    "    \"\"\"\n",
    "    utility function to Load Image From the path provided\n",
    "    \n",
    "    paramter\n",
    "    ---------\n",
    "    \n",
    "    image_folder: str\n",
    "        image folder path containing Images of labels provided in parameter\n",
    "        \n",
    "    label: str\n",
    "        label Name of all Images under given image_folder\n",
    "    \n",
    "    returns\n",
    "    --------\n",
    "        List[object]\n",
    "        List of image object defined in function get_pickel_object\n",
    "    \"\"\"\n",
    "    global IMAGE_SIZE\n",
    "   \n",
    "    folder_list = list(os.path.split(image_folder))\n",
    "    print('Initialting '+label+' Data loading...')\n",
    "    image_files = os.listdir(image_folder)\n",
    "    ls = []\n",
    "    progressbar = '============================='\n",
    "    PROGRESS_BAR_SIZE = 500\n",
    "    ctr, n = 1, len(image_files)\n",
    "    for imagename in image_files:\n",
    "\n",
    "        image_path = os.path.join(*(folder_list+[imagename]))\n",
    "\n",
    "        if not os.path.isfile(image_path):\n",
    "            continue\n",
    "        try:\n",
    "            extension = os.path.splitext(imagename)[1]\n",
    "            image = tfpreprocess.load_img(image_path, color_mode=\"grayscale\",\n",
    "                                        target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "            data = tfpreprocess.img_to_array(image).reshape(IMAGE_SIZE, IMAGE_SIZE).astype(\"float32\")\n",
    "            obj = get_pickel_object(image_path, extension, data, IMAGE_SIZE, IMAGE_SIZE, label)\n",
    "            ls.append(obj)\n",
    "        except IOError as e:\n",
    "            print('skipping file IO Error occured '+str(e))\n",
    "        \n",
    "        if ctr%PROGRESS_BAR_SIZE == 0:\n",
    "            info = progressbar+' {0}/{1} '.format(ctr, n) + progressbar\n",
    "            print(info)\n",
    "        \n",
    "        ctr += 1\n",
    "    \n",
    "    if ctr <= n:\n",
    "        info = progressbar+' {0}/{1} '.format(ctr, n) + progressbar\n",
    "        print(info)\n",
    "    print('\\n{0} data loaded\\n'.format(label))\n",
    "    return ls\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to load the data If pickle file found then load from pickle file directly if\n",
    "force parameter is False else override existing pickle file and return dictionary Label to dataset mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_disk(pathlist: List[str], picklefilefolder: str, force=False)-> Tuple[Dict[str,Tuple[str, List[object]]], int]:\n",
    "    \"\"\"\n",
    "    Load all Images from given path List\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "        \n",
    "    pathlist: List[str]\n",
    "        list of paths of folder from which images need to loaded\n",
    "    \n",
    "    picklefilefolder: str\n",
    "        pickle folder path If pickled file found then load from pickle file it will save time\n",
    "    \n",
    "    force: bool\n",
    "        If set to True then Image load from folders not from pickle file\n",
    "    \n",
    "    returns\n",
    "    ----------\n",
    "        Tuple[Dict[str List[object]], int]\n",
    "        Tuple of dictionary of label to list of image object mapping , maximum number of Images of particular Label\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    max_size = 0\n",
    "    for path in pathlist:\n",
    "    \n",
    "        label = os.path.split(path)[-1]\n",
    "        pickefilepath = os.path.join(picklefilefolder, label+'.pickle')\n",
    "        \n",
    "        if os.path.exists(pickefilepath) and not force:\n",
    "            print(pickefilepath+' is already exist loading from pickle file....')\n",
    "            \n",
    "            with open(pickefilepath, 'rb') as pf:\n",
    "                resultdataset = pickle.load(pf) \n",
    "                res[label] = (pickefilepath, resultdataset)\n",
    "        else:\n",
    "            \n",
    "            resultdataset = load_image_from_path(path, label)\n",
    "            res[label] = (pickefilepath, resultdataset)\n",
    "        \n",
    "        max_size = max(max_size, len(resultdataset))\n",
    "    \n",
    "    return (res, max_size)\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save data into pickle file which having the size specified in parameter If\n",
    "pickle file already present then override it provided override to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_into_pickle_file(dataset: List[object], picklefilepath: str, datasize: int,\n",
    "                                              data_size_modification=False, override=False)->None:\n",
    "    \"\"\"\n",
    "    save data into pickle file\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "    \n",
    "    dataset: List[object]\n",
    "        list of image objects\n",
    "    \n",
    "    picklefilepath: str\n",
    "        pickle file with path where pickle file need to save\n",
    "    \n",
    "    datasize: int\n",
    "        data size of image object list need to be saved in pickle file\n",
    "    \n",
    "    data_size_modification: bool\n",
    "        If true then dataset modification takes place If list of image object\n",
    "        less then given datasize augumented image data will be added, if more\n",
    "        data found then truncated the data and saved into pickle file\n",
    "    \n",
    "    override: bool\n",
    "        if set to true then if pickle file already existing then override it\n",
    "    \n",
    "    return\n",
    "    -------\n",
    "        None\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        n = len(dataset)\n",
    "        if data_size_modification:\n",
    "            if n < datasize:\n",
    "                print('\\nless Image found addeing more augumented Data...')\n",
    "                # make last argument true If you want to save Image in same directory\n",
    "                aug_list = image_augumentation_generator(dataset, datasize, False)\n",
    "                dataset.extend(aug_list)\n",
    "            elif n > datasize:\n",
    "                print('more Image found truncationg Data...')\n",
    "                dataset = dataset[:datasize]\n",
    "            else:\n",
    "                print('Data size is equal to Expected size..')\n",
    "        else:\n",
    "            print('No modification of Data take place.')\n",
    "        \n",
    "        if os.path.exists(picklefilepath) and not override:\n",
    "            print('File {0} already exists skipping...'.format(picklefilepath))\n",
    "            return\n",
    "        with open(picklefilepath, 'wb') as pf:\n",
    "            pickle.dump(dataset, pf, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('Data saved successfully to {0} path data size = {1}'.format(picklefilepath, datasize))\n",
    "    except:\n",
    "        raise\n",
    "#End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return dataset dictionary label to dataset mapping with fixed size if size is specified then\n",
    "If override is specified it wite pickle file to disk if size if not specified then map the\n",
    "data as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(datadict:Dict[str, Tuple[str, List[object]]], size: int,\n",
    "                data_size_modification=False, override=False)->Dict[str, List[object]]:\n",
    "    \"\"\"\n",
    "     Get Data of specified size in parameter\n",
    "     paramter\n",
    "     --------\n",
    "     datadict: Dict[str, Tuple[str, List[object]]]\n",
    "         data dictionary obtained from function load_dataset_from_disk  contain key as label\n",
    "         and value as tuple of picklepath and list of image object\n",
    "        \n",
    "    size: int\n",
    "        size of each laebl data needed only effective if data_size_modification is true\n",
    "    \n",
    "    data_size_modification: bool\n",
    "        if set to true then only list of Image object modification appending augumented data\n",
    "        or truncating data is allowed other wise skipping it\n",
    "    \n",
    "    override: bool\n",
    "         If true then pickle file is overriden with new One\n",
    "        \n",
    "    return\n",
    "    ---------\n",
    "        Dict[str, List[object]]\n",
    "        dictionary with label to List of imae objects\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Max size of Data {0}'.format(size))\n",
    "    datasets = {}\n",
    "    for label, data in datadict.items(): # key is lebel and data is tuple of pickle file path and dataset\n",
    "        try:\n",
    "            picklepath, dataset = data[0], data[1]\n",
    "            n = len(dataset)\n",
    "            print('Label {0} Data size {1}'.format(label, n))\n",
    "            save_dataset_into_pickle_file(dataset, picklepath, size, data_size_modification, override)\n",
    "            datasets[label] = dataset\n",
    "        except:\n",
    "            print(data)\n",
    "    return datasets\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading train data from disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large\\A.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\B.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\C.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\D.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\E.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\F.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\G.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\H.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\I.pickle is already exist loading from pickle file....\n",
      "notMNIST_large\\J.pickle is already exist loading from pickle file....\n"
     ]
    }
   ],
   "source": [
    "dataset_dict_train, max_train_size  = load_dataset_from_disk(pathlist=PATH_TRAIN,\n",
    "                                                             picklefilefolder=TRAIN_FOLDER,\n",
    "                                                             force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading test data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small\\A.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\B.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\C.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\D.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\E.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\F.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\G.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\H.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\I.pickle is already exist loading from pickle file....\n",
      "notMNIST_small\\J.pickle is already exist loading from pickle file....\n"
     ]
    }
   ],
   "source": [
    "dataset_dict_test, max_test_size  = load_dataset_from_disk(pathlist=PATH_TEST,\n",
    "                                                           picklefilefolder=TEST_FOLDER,\n",
    "                                                           force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the loaded train data into pickle file if override=True else make modify\n",
    "the data to equal length if data_size_modification=True is set else return the\n",
    "data dict label to data object list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max size of Data 52912\n",
      "Label A Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\A.pickle already exists skipping...\n",
      "Label B Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\B.pickle already exists skipping...\n",
      "Label C Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\C.pickle already exists skipping...\n",
      "Label D Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\D.pickle already exists skipping...\n",
      "Label E Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\E.pickle already exists skipping...\n",
      "Label F Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\F.pickle already exists skipping...\n",
      "Label G Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\G.pickle already exists skipping...\n",
      "Label H Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\H.pickle already exists skipping...\n",
      "Label I Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\I.pickle already exists skipping...\n",
      "Label J Data size 52912\n",
      "Data size is equal to Expected size..\n",
      "File notMNIST_large\\J.pickle already exists skipping...\n"
     ]
    }
   ],
   "source": [
    "train_dict = get_dataset(datadict=dataset_dict_train, size=max_train_size, data_size_modification=True, override=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the loaded test data into pickle file if override=True else make modify\n",
    "the data to equal length if data_size_modification=True is set else return the\n",
    "data dict label to data object list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max size of Data 1873\n",
      "Label A Data size 1872\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\A.pickle already exists skipping...\n",
      "Label B Data size 1873\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\B.pickle already exists skipping...\n",
      "Label C Data size 1873\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\C.pickle already exists skipping...\n",
      "Label D Data size 1873\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\D.pickle already exists skipping...\n",
      "Label E Data size 1873\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\E.pickle already exists skipping...\n",
      "Label F Data size 1872\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\F.pickle already exists skipping...\n",
      "Label G Data size 1872\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\G.pickle already exists skipping...\n",
      "Label H Data size 1872\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\H.pickle already exists skipping...\n",
      "Label I Data size 1872\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\I.pickle already exists skipping...\n",
      "Label J Data size 1872\n",
      "No modification of Data take place.\n",
      "File notMNIST_small\\J.pickle already exists skipping...\n"
     ]
    }
   ],
   "source": [
    "test_dict = get_dataset(datadict=dataset_dict_test, size=max_test_size, data_size_modification=False, override=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve all the class Names and save into list from train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LIST = list(train_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_map_and_inverse_map(class_list: List[str])-> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    label encoder and decoder\n",
    "    paramter\n",
    "    --------\n",
    "    \n",
    "    class_list: List[str]\n",
    "        list of Labels\n",
    "    \n",
    "    return \n",
    "    --------\n",
    "        Tuple[Dict[str, int], Dict[int, str]]\n",
    "        tuple of (label to integer dictionary, integer to label dictionary)\n",
    "    \"\"\"\n",
    "    class_map = {x: class_list.index(x) for x in class_list}\n",
    "    class_inv_map = {class_list.index(x): x for x in class_list}\n",
    "    return (class_map, class_inv_map)\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save class Label to integer and Inverse map integer to class Label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAP, CLASS_INV_MAP = get_class_map_and_inverse_map(CLASS_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all dataset into single one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(data_train: Dict[str, List[object]], size: int, label_map: Dict[str, int], validation_size=None):\n",
    "    \"\"\"\n",
    "    merge Multiple Data Into single numpy array\n",
    "    paramter\n",
    "    ----------\n",
    "    \n",
    "    data_train: Dict[str, List[object]]\n",
    "        mapping label to data object list\n",
    "    \n",
    "    size: int\n",
    "        size of each label data sample if -1 given take all the data else truncate the rest of the data\n",
    "    \n",
    "    label_map: Dict[str, int]\n",
    "        label Incoded mapping of label to number\n",
    "    \n",
    "    validation_size: int  (optional)\n",
    "        If provided then create that much size validation set from truncated data after size provided in args\n",
    "    \n",
    "    returns\n",
    "    -----------\n",
    "    Tuple[np.array, np.array, np.array, np.array]\n",
    "        numpy array of dataset, dataset label, validation set, validation label \n",
    "        Ignore validation set if validation_size None provided as it return empty list\n",
    "    \"\"\"\n",
    "    def get_data(x, label):\n",
    "        lb = np.zeros(len(label_map), dtype=int)\n",
    "        lb[label_map[x['label']]] = 1\n",
    "        label.append(lb)\n",
    "        return x['data']\n",
    "    # End\n",
    "    \n",
    "    datasets, labels, dataset_val, label_val = [], [], [], []\n",
    "    for label, data in data_train.items():\n",
    "        n = len(data)\n",
    "        label = []\n",
    "        dataset = list(map(lambda x : get_data(x, label), data))\n",
    "        if size < 0:\n",
    "            datasets.extend(dataset)\n",
    "            labels.extend(label)\n",
    "            continue\n",
    "        \n",
    "        if validation_size is not None and (size+validation_size) > n:\n",
    "            raise Exception('Available data is {0} required Data is {1}'.format(n, size+validation_size))\n",
    "        \n",
    "        if validation_size is None and size > n:\n",
    "            raise Exception('Available data is {0} required Data is {1}'.format(n, size))\n",
    "        \n",
    "        datasets.extend(dataset[:size])\n",
    "        labels.extend(label[:size])\n",
    "        \n",
    "        if validation_size is not None:\n",
    "            dataset_val.extend(dataset[size:size+validation_size])\n",
    "            label_val.extend(label[size:size+validation_size])\n",
    "    datasets, labels = np.array(datasets), np.array(labels)\n",
    "    dataset_val, label_val = np.array(dataset_val), np.array(label_val)\n",
    "    \n",
    "    return (datasets, labels, dataset_val, label_val)\n",
    "# End    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random shuffuling of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset: np.array, labels: np.array)->(np.array, np.array):\n",
    "    \"\"\"\n",
    "    random shuffle of dataset\n",
    "    \n",
    "    parameter\n",
    "    ----------\n",
    "        dataset: np.array (numpy array)\n",
    "            dataset neet to shuffle\n",
    "        \n",
    "        labels: np.array (numpy array)\n",
    "            label assosiate with dataset\n",
    "    return np.array, np.array\n",
    "        random shuffled dataset and labels\n",
    "    \"\"\"\n",
    "    global SEED\n",
    "    if dataset.shape[0] != labels.shape[0]:\n",
    "        raise Exception('labels and data set length mismatch label len:- {0} dataset len:- {1}'.format(dataset.shape[0],labels.shape[0]))\n",
    "    np.random.seed(SEED)\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    sh_dataset = dataset[permutation, :, :]\n",
    "    sh_label = labels[permutation, :]\n",
    "    return sh_dataset, sh_label\n",
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix how many number of each classs sample need to be in training and validation sets\n",
    "negaive number represent all the data available in that case it will skip validation \n",
    "data and it comes out to be empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE_PER_LABEL = 30000 # take TRAIN_SIZE_PER_LABEL data for training\n",
    "VALIDATION_SIZE_PER_LABEL = 20000 # take VALIDATION_SIZE_PER_LABEL amount for validation \n",
    "TEST_SIZE = -1 # Negative Number represent All Data will be taken "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get merged training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, training_label, validation_set, validation_label = merge_dataset(data_train=train_dict,\n",
    "                                                                               size=TRAIN_SIZE_PER_LABEL,\n",
    "                                                                               label_map=CLASS_MAP,\n",
    "                                                                               validation_size=VALIDATION_SIZE_PER_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, test_label, _, _ = merge_dataset(data_train=test_dict, size=TEST_SIZE, label_map=CLASS_MAP, validation_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get random shuffled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, training_label = randomize(training_set, training_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get random shuffled validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set, validation_label = randomize(validation_set, validation_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get random shuffled test data not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set, test_label = randomize(test_set, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape (300000, 28, 28) training label shape:- (300000, 10)\n",
      "validation data shape (200000, 28, 28) validation label shape:- (200000, 10)\n",
      "test data shape (18724, 28, 28) test label shape:- (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "print('training data shape {0} training label shape:- {1}'.format(training_set.shape, training_label.shape))\n",
    "print('validation data shape {0} validation label shape:- {1}'.format(validation_set.shape, validation_label.shape))\n",
    "print('test data shape {0} test label shape:- {1}'.format(test_set.shape, test_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random check from training Data check If valid random sampling occured or not\n",
    "def random_check(dataset: np.array, label: np.array, index: int):\n",
    "    plt.imshow(dataset[index].astype('uint8'), cmap=plt.get_cmap('gray'))\n",
    "    print('label is ', CLASS_INV_MAP[np.argmax(label[index])])\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_check(training_set, training_label, 23 )\n",
    "#random_check(validation_set, validation_label, 116)\n",
    "#random_check(test_set, test_label, 1870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_datasets(data: Dict[str, np.array], filepath: str, force=False)->None:\n",
    "    \"\"\"\n",
    "    pickle all the important data\n",
    "    \n",
    "    parameter\n",
    "    ---------\n",
    "        data: Dict[str, np.array]\n",
    "            data dictionary data name to data mapping\n",
    "        \n",
    "        filepath: str\n",
    "            file path where to pickle the data\n",
    "        \n",
    "        force: bool (optional)\n",
    "            if set to true then override the already pickled file\n",
    "    \n",
    "    return\n",
    "    -------\n",
    "        None\n",
    "            \n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath) and not force:\n",
    "        print('dataset already present skipping')\n",
    "    print('writing dataset to {0} file..'.format(filepath))\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suffix = 'all' if TEST_SIZE < 0 else TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'trainingSet'+str(TRAIN_SIZE_PER_LABEL): training_set,\n",
    "    'trainingLabel'+str(TRAIN_SIZE_PER_LABEL): training_label,\n",
    "    'validationSet'+str(VALIDATION_SIZE_PER_LABEL): validation_set,\n",
    "    'validationLabel'+str(VALIDATION_SIZE_PER_LABEL): validation_label,\n",
    "    'testSet'+str(test_suffix): test_set,\n",
    "    'testLabel'+str(test_suffix): test_label,\n",
    "    'classLabelEncodingMapping': CLASS_MAP\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds_pickle_filepath = 'tr_set_{0}_val_{1}_test_{2}.pickle'.format(TRAIN_SIZE_PER_LABEL,\n",
    "                                                                 VALIDATION_SIZE_PER_LABEL,\n",
    "                                                                test_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing dataset to tr_set_30000_val_20000_test_all.pickle file..\n"
     ]
    }
   ],
   "source": [
    "pickle_datasets(dataset,ds_pickle_filepath, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not in use just to create sample data to check for merge_dataset function\n",
    "skip it, not in use in any ways, i am keeping this to test in case something\n",
    "was not right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_gen(N, size, label_list):\n",
    "    \n",
    "    get_d = lambda l, n : [{'data': np.random.randint(1, 100, size), 'label': l} for i in range(n)]\n",
    "    dat = {l:get_d(l, N) for l in label_list}\n",
    "    return dat\n",
    "# End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = test_data_gen(5, (2, 3), CLASS_LIST)\n",
    "tr, lbt, val_d, val_lb = merge_dataset(sample_data, 4, CLASS_MAP, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_val, r_val_lab = randomize(val_d, val_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 2, 3) (40, 10) (10, 2, 3) (10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(tr.shape, lbt.shape, val_d.shape, val_lb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
